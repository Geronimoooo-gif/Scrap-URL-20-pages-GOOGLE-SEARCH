import streamlit as st
import pandas as pd
import requests
import time
from datetime import datetime
import logging
import io

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ValueSerpAPI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.valueserp.com/search"
    
    def search(self, query, start=0):
        """Effectue une recherche via l'API ValueSerp"""
        params = {
            'api_key': self.api_key,
            'q': query,
            'location': 'France',
            'gl': 'fr',
            'hl': 'fr',
            'google_domain': 'google.fr',
            'start': start,
            'num': 100,
            'output': 'json',
            'include_html': 'false',
            'device': 'desktop'
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la requ√™te API: {str(e)}")
            return None

def scrape_google_urls(query, max_results=200, progress_bar=None):
    """Scrape les r√©sultats Google via ValueSerp"""
    results = []
    
    api_key = st.secrets["VALUESERP_API_KEY"]
    api = ValueSerpAPI(api_key)
    
    # Calculer le nombre de requ√™tes n√©cessaires (100 r√©sultats par requ√™te)
    num_requests = (max_results + 99) // 100  # Arrondi sup√©rieur
    
    for i in range(num_requests):
        start = i * 100
        if progress_bar:
            progress = (start + 100) / max_results
            progress_bar.progress(min(progress, 1.0))
        
        logger.info(f"R√©cup√©ration des r√©sultats {start+1} √† {min(start+100, max_results)}")
        
        response_data = api.search(query, start=start)
        
        if not response_data:
            logger.error("Pas de r√©ponse de l'API")
            break
            
        organic_results = response_data.get('organic_results', [])
        
        if not organic_results:
            logger.warning(f"Aucun r√©sultat trouv√© √† partir de l'index {start}")
            break
            
        for position, result in enumerate(organic_results, start=start+1):
            if len(results) >= max_results:
                break
            url = result.get('link')
            if url:
                results.append({
                    "Position": position,
                    "URL": url
                })
        
        if len(results) >= max_results:
            break
            
        logger.info(f"Trouv√© {len(organic_results)} r√©sultats pour la page {i+1}")
        time.sleep(1)
    
    logger.info(f"Scraping termin√©. Nombre total de r√©sultats: {len(results)}")
    return results[:max_results]

def create_excel_with_multiple_sheets(dataframes, filename):
    """Cr√©e un fichier Excel avec plusieurs onglets"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for sheet_name, df in dataframes.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    return output.getvalue()

def main():
    st.title("üîç Scraper Google Search via ValueSerp")
    
    if "VALUESERP_API_KEY" not in st.secrets:
        st.error("Cl√© API ValueSerp manquante. Veuillez configurer vos secrets Streamlit.")
        return
    
    # Liste des villes
    default_cities = """Paris
Paris 1er arrondissement
Paris 2e arrondissement
Paris 3e arrondissement
Paris 4e arrondissement
Paris 5e arrondissement
Paris 6e arrondissement
Paris 7e arrondissement
Paris 8e arrondissement
Paris 9e arrondissement
Paris 10e arrondissement
Paris 11e arrondissement
Paris 12e arrondissement
Paris 13e arrondissement
Paris 14e arrondissement
Paris 15e arrondissement
Paris 16e arrondissement
Paris 17e arrondissement
Paris 18e arrondissement
Paris 19e arrondissement
Paris 20e arrondissement
Marseille
Lyon
Toulouse
Nice
Nantes
Strasbourg
Montpellier
Bordeaux
Lille
Rennes
Reims
Saint-Etienne
Toulon
Le Havre
Grenoble
Dijon
Angers
Nimes
Villeurbanne
Clermont-Ferrand
Saint-Denis
Le Mans
Aix-en-Provence
Brest
Tours
Amiens
Limoges
Annecy
Perpignan
Boulogne-Billancourt
Metz
Besancon
Orleans
Saint-Denis
Rouen
Argenteuil
Mulhouse
Montreuil
Caen
Nancy
Saint-Paul
Roubaix
Tourcoing
Nanterre
Vitry-sur-Seine
Avignon
Creteil
Poitiers
Dunkerque
Asnieres-sur-Seine
Courbevoie
Versailles
Colombes
Fort-de-France
Aulnay-sous-Bois
Saint-Pierre
Rueil-Malmaison
Pau
Aubervilliers
Champigny-sur-Marne
Le Tampon
Antibes
Saint-Maur-des-Fosses
Cannes
Drancy
Merignac
Saint-Nazaire
Colmar
Issy-les-Moulineaux
Noisy-le-Grand
Evry-Courcouronnes
Levallois-Perret
Troyes
Neuilly-sur-Seine
Sarcelles
Venissieux
Clichy
Pessac
Ivry-sur-Seine
Cergy
Quimper
La Rochelle
Beziers
Ajaccio
Saint-Quentin
Niort
Villejuif
Hyeres
Pantin
Chambery
Le Blanc-Mesnil
Lorient
Les Abymes
Montauban
Sainte-Genevieve-des-Bois
Suresnes
Meaux
Valence
Beauvais
Cholet
Chelles
Bondy
Frejus
Clamart
Narbonne
Bourg-en-Bresse
Fontenay-sous-Bois
Bayonne
Sevran
Antony
Maisons-Alfort
La Seyne-sur-Mer
Epinay-sur-Seine
Montrouge
Saint-Herblain
Calais
Vincennes
Macon
Villepinte
Martigues
Bobigny
Cherbourg-en-Cotentin
Vannes
Massy
Brive-la-Gaillarde
Arles
Corbeil-Essonnes
Saint-Andre
Saint-Ouen-sur-Seine
Albi
Belfort
Evreux
La Roche-sur-Yon
Saint-Malo
Bagneux
Chateauroux
Noisy-le-Sec
Salon-de-Provence
Le Cannet
Vaulx-en-Velin
Livry-Gargan
Angouleme
Sete
Puteaux
Thionville
Rosny-sous-Bois
Saint-Laurent-du-Maroni
Alfortville
Istres
Gennevilliers
Wattrelos
Talence
Blois
Tarbes
Castres
Garges-les-Gonesse
Saint-Brieuc
Arras
Douai
Compiegne
Melun
Reze
Saint-Chamond
Bourgoin-Jallieu
Gap
Montelimar
Thonon-les-Bains
Draguignan
Chartres
Joue-les-Tours
Saint-Martin-dHeres
Villefranche-sur-Saone
Chalon-sur-Saone
Mantes-la-Jolie
Colomiers
Anglet
Pontault-Combault
Poissy
Savigny-sur-Orge
Bagnolet
Lievin
Nevers
Gagny
Le Perreux-sur-Marne
Stains
Chalons-en-Champagne
Conflans-Sainte-Honorine
Montlucon
Palaiseau
Laval
Saint-Priest
LHay-les-Roses
Brunoy
Chatillon
Sainte-Marie
Bastia
Lens
Chambery
Saint-Benoit
Le Port
Saint-Leu
Noumea"""
    
    # Interface utilisateur
    with st.container():
        col1, col2 = st.columns(2)
        
        with col1:
            query = st.text_input(
                "Entrez votre terme de recherche",
                value="",
                placeholder="Exemple: avocat",
                help="Tapez votre terme de recherche principal"
            )
        
        with col2:
            cities = st.text_area(
                "Liste des villes (une par ligne)",
                value=default_cities,
                height=100,
                help="Entrez les villes, une par ligne"
            )
        
        max_results = st.select_slider(
            "Nombre de r√©sultats √† r√©cup√©rer par ville",
            options=[10, 20, 30, 50, 100, 200],
            value=100,
            help="Choisissez le nombre de r√©sultats Google √† r√©cup√©rer par ville"
        )
        
        # Calcul des co√ªts
        cities_list = [city.strip() for city in cities.split('\n') if city.strip()]
        num_requests_per_city = (max_results + 99) // 100
        total_requests = num_requests_per_city * len(cities_list)
        cost_per_request = 0.001
        estimated_cost = total_requests * cost_per_request
        
        # Informations de co√ªt dans la sidebar
        st.sidebar.title("Estimation des co√ªts")
        st.sidebar.write(f"Nombre de villes: {len(cities_list)}")
        st.sidebar.write(f"Requ√™tes par ville: {num_requests_per_city}")
        st.sidebar.write(f"Total requ√™tes: {total_requests}")
        st.sidebar.write(f"Co√ªt estim√©: ${estimated_cost:.3f}")
        
        search_button = st.button("üîç Lancer les recherches")
        
        if search_button:
            if not query or not cities_list:
                st.error("Veuillez entrer un terme de recherche et au moins une ville")
                return
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Dictionnaire pour stocker les r√©sultats par ville
            all_results = {}
            
            for i, city in enumerate(cities_list):
                full_query = f"{query} {city}"
                status_text.text(f"Recherche en cours pour : {full_query}")
                
                data = scrape_google_urls(full_query, max_results, progress_bar)
                if data:
                    df = pd.DataFrame(data)[["Position", "URL"]]
                    all_results[full_query] = df
                
                progress = (i + 1) / len(cities_list)
                progress_bar.progress(progress)
                
            if all_results:
                st.success(f"Recherches termin√©es ! R√©sultats trouv√©s pour {len(all_results)} villes.")
                
                # Cr√©ation du fichier Excel
                excel_data = create_excel_with_multiple_sheets(all_results, "resultats_recherche.xlsx")
                
                # Bouton de t√©l√©chargement
                st.download_button(
                    label="üì• T√©l√©charger les r√©sultats (Excel)",
                    data=excel_data,
                    file_name=f"recherche_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                
                # Affichage des aper√ßus
                for query_city, df in all_results.items():
                    with st.expander(f"Aper√ßu des r√©sultats pour : {query_city}"):
                        st.dataframe(df)
                
                # Statistiques dans la sidebar
                st.sidebar.write("---")
                st.sidebar.write("Statistiques de la recherche")
                st.sidebar.write(f"Villes trait√©es: {len(all_results)}")
                st.sidebar.write(f"Co√ªt r√©el: ${(total_requests * cost_per_request):.3f}")
            else:
                st.error("Aucun r√©sultat trouv√©.")

if __name__ == "__main__":
    main()
